{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6641dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "import src.seed as seed\n",
    "import src.models as models\n",
    "import src.functions as fn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "device = seed.device\n",
    "generator = seed.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_test, y_test = fn.load_cifar_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee97e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_muon_layer_sharpness(model, opt_muon, criterion, X, y, generator,\n",
    "                             subsample_dim=1024, iters=30, tol=1e-4):\n",
    "    # get muon weight matrices\n",
    "    ps = [p for g in opt_muon.param_groups for p in g[\"params\"]]\n",
    "    # keep only 2D (weight matrices)\n",
    "    muon_ws = [p for p in ps if p.ndim == 2]\n",
    "    if len(muon_ws) == 0:\n",
    "        raise ValueError(\"No 2D Muon parameters found in opt_muon.\")\n",
    "\n",
    "    # subsample\n",
    "    n = X.shape[0]\n",
    "    m = min(subsample_dim, n)\n",
    "    idx = torch.randperm(n, device=X.device, generator=generator)[:m]\n",
    "    Xs, ys = X[idx], y[idx]\n",
    "\n",
    "    # forward once\n",
    "    outputs = model(Xs)\n",
    "    loss = criterion(outputs, ys)\n",
    "\n",
    "    def power_iteration_for_param(W):\n",
    "        # grad wrt W\n",
    "        (gW,) = torch.autograd.grad(loss, W, create_graph=True, retain_graph=True)\n",
    "        g_flat = gW.reshape(-1)\n",
    "        dim = g_flat.numel()\n",
    "        device = g_flat.device\n",
    "\n",
    "        def Hv(v):\n",
    "            # Hessian-vector product wrt W only\n",
    "            (hW,) = torch.autograd.grad(g_flat @ v, W, retain_graph=True)\n",
    "            return hW.reshape(-1)\n",
    "\n",
    "        v = torch.randn(dim, device=device, generator=generator)\n",
    "        v = v / (v.norm() + 1e-12)\n",
    "\n",
    "        eig_old = 0.0\n",
    "        for _ in range(iters):\n",
    "            w = Hv(v)\n",
    "            eig = (v @ w).item()\n",
    "            v = w / (w.norm() + 1e-12)\n",
    "\n",
    "            if abs(eig - eig_old) / (abs(eig_old) + 1e-12) < tol:\n",
    "                break\n",
    "            eig_old = eig\n",
    "\n",
    "        w = Hv(v)\n",
    "        return (v @ w).item()\n",
    "\n",
    "    # compute per-muon-layer sharpness and take max\n",
    "    lambdas = [power_iteration_for_param(W) for W in muon_ws]\n",
    "    return max(lambdas)\n",
    "\n",
    "def train_muon_model(model, opt_muon, opt_adam, criterion, epochs, accuracy, \n",
    "                     X, y, X_test, y_test, output_dir, generator):\n",
    "    \"\"\"Trains the provided model with the specified optimizer and criterion for \n",
    "    a set number of epochs or until the desired accuracy is reached. Records \n",
    "    training loss, training accuracy, test accuracy, and sharpness metrics at \n",
    "    each epoch.\n",
    "\n",
    "    Args:\n",
    "        model (_type_): The neural network model to train\n",
    "        opt_muon (_type_): The Muon optimizer used for training\n",
    "        opt_adam (_type_): The Adam optimizer used for training\n",
    "        criterion (_type_): The loss function used for training\n",
    "        epochs (_type_): The maximum number of training epochs\n",
    "        accuracy (_type_): The target accuracy to stop training early\n",
    "        X (_type_): Training input data\n",
    "        y (_type_): Training target labels\n",
    "        X_test (_type_): Test input data\n",
    "        y_test (_type_): Test target labels\n",
    "        output_dir (_type_): Directory to save output files\n",
    "        generator (_type_): Random number generator for reproducibility\n",
    "    \"\"\"\n",
    "    print(f\"Training {model.__class__.__name__} with \" +\n",
    "          f\"{opt_muon.__class__.__name__} and learning rate \" +\n",
    "          f\"{opt_muon.param_groups[0]['lr']} for {epochs} epochs.\")\n",
    "\n",
    "    learning_rate = opt_muon.param_groups[0]['lr']\n",
    "    momentum = opt_muon.param_groups[0].get('momentum', 0.0)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_losses = np.full(epochs, np.nan)\n",
    "    train_accuracies = np.full(epochs, np.nan)\n",
    "    test_accuracies = np.full(epochs, np.nan)\n",
    "    H_sharps = np.full(epochs, np.nan)\n",
    "    A_sharps = np.full(epochs, np.nan)\n",
    "\n",
    "    if isinstance(criterion, nn.MSELoss):\n",
    "        y_loss = torch.nn.functional.one_hot(\n",
    "            y, num_classes=model.num_labels).float().to(device)\n",
    "       \n",
    "    else:\n",
    "        y_loss = y.to(device)\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    train_acc = 0.0\n",
    "    epoch = 0\n",
    "\n",
    "    while train_acc < accuracy and epoch < epochs :\n",
    "\n",
    "        opt_muon.zero_grad(set_to_none=True)\n",
    "        opt_adam.zero_grad(set_to_none=True)\n",
    "\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y_loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        opt_muon.step()\n",
    "        opt_adam.step()\n",
    "        \n",
    "        train_losses[epoch] = loss.item()\n",
    "        \n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            H_sharps[epoch] = max_muon_layer_sharpness(\n",
    "                model, opt_muon, criterion, X, y_loss, generator=generator\n",
    "            )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            train_preds = outputs.argmax(dim=1)\n",
    "            test_preds = model(X_test).argmax(dim=1)\n",
    "            train_acc = (train_preds == y).float().mean().item()\n",
    "            test_acc = (test_preds == y_test).float().mean().item()\n",
    "            train_accuracies[epoch] = train_acc\n",
    "            test_accuracies[epoch] = test_acc\n",
    "        model.train()\n",
    "\n",
    "        if (epoch+1) % 1000 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, \" +\n",
    "                  f\"Time: {round(((time.time() - start) / 60), 2)}, \" +\n",
    "                  f\"Train Acc: {train_accuracies[epoch]:.4f}, \" +\n",
    "                  f\"Test Acc: {test_accuracies[epoch]:.4f}, \")\n",
    "        epoch += 1\n",
    "\n",
    "    metadata, output_data = fn.setup_output_files(output_dir)\n",
    "    model_id = metadata.shape[0] + 1\n",
    "\n",
    "    metadata.loc[metadata.shape[0]] ={\n",
    "        \"model_id\": model_id,\n",
    "        \"model_type\": model.__class__.__name__,\n",
    "        \"activation_function\": model.activation.__name__,\n",
    "        \"optimizer\": opt_muon.__class__.__name__,\n",
    "        \"criterion\": criterion.__class__.__name__,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"momentum\": momentum,\n",
    "        \"num_epochs\": epochs,\n",
    "        \"time_minutes\": round((time.time() - start) / 60, 2),\n",
    "    }\n",
    "\n",
    "    output_data = pd.concat([output_data, pd.DataFrame({\n",
    "        \"model_id\": np.ones_like(train_losses) * model_id,\n",
    "        \"epoch\": np.arange(1, epochs + 1),\n",
    "        \"train_loss\": train_losses,\n",
    "        \"sharpness_H\": H_sharps.round(4),\n",
    "        \"sharpness_A\": A_sharps.round(4),\n",
    "        \"test_accuracy\": test_accuracies,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    fn.save_output_files(metadata, output_data, output_dir)\n",
    "\n",
    "class MLP4(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_labels, activation):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers_size = hidden_layer_size\n",
    "        self.num_labels = num_labels\n",
    "        self.activation = activation\n",
    "\n",
    "        self.h1  = nn.Linear(input_size,  hidden_layer_size)\n",
    "        self.h2  = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.h3  = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.h4  = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.out = nn.Linear(hidden_layer_size, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.h1(x))\n",
    "        x = F.relu(self.h2(x))\n",
    "        x = F.relu(self.h3(x))\n",
    "        x = F.relu(self.h4(x))\n",
    "        return self.out(x)\n",
    "\n",
    "output_dir = \"eos/muon_MJ\"\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_layer_size = 170\n",
    "num_labels = 10\n",
    "activation = F.relu\n",
    "model = MLP4(input_size, hidden_layer_size, num_labels, activation)\n",
    "criterion = nn.MSELoss()\n",
    "lr=1e-3\n",
    "\n",
    "# Muon only for 2D weight matrices of inner layers (h2, h3)\n",
    "muon_params = [model.h2.weight, model.h3.weight]\n",
    "\n",
    "# AdamW for everything else (including biases + edge hidden + head)\n",
    "adamw_params = (\n",
    "    [model.h1.weight, model.h4.weight, model.out.weight] +\n",
    "    [model.h1.bias, model.h2.bias, model.h3.bias, model.h4.bias, model.out.bias]\n",
    ")\n",
    "\n",
    "opt_muon = torch.optim.Muon(muon_params, lr=lr)\n",
    "opt_adamw = torch.optim.AdamW(adamw_params, lr=lr)\n",
    "\n",
    "train_muon_model(\n",
    "    model=model,\n",
    "    opt_muon=opt_muon,\n",
    "    opt_adam=opt_adamw,\n",
    "    criterion=criterion,\n",
    "    epochs=500,\n",
    "    accuracy=0.9999999,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    output_dir=output_dir,\n",
    "    generator=generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.delete_model_data([1],output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "md, out = fn.load_output_files(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b57e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output_data(metadata, output, model_id):\n",
    "    metadata = metadata[metadata['model_id']==model_id]\n",
    "    output = output[output['model_id']==model_id]\n",
    "    \n",
    "    xs = np.arange(metadata['num_epochs'].iloc[0])\n",
    "    losses = output['train_loss']\n",
    "    sharpness_H = output['sharpness_H']\n",
    "    sharpness_A = output['sharpness_A']\n",
    "    train_accuracy = output['train_accuracy']\n",
    "    test_accuracy = output['test_accuracy']\n",
    "    learning_rate = metadata['learning_rate'].iloc[0]\n",
    "    sharpness_H_lim = 2 * (1 + 0.9)  / ((1 - 0.9) * learning_rate)\n",
    "\n",
    "    fig = make_subplots(rows = 2, cols = 1, \n",
    "                        specs=[[{\"secondary_y\": True}],\n",
    "                               [{\"secondary_y\": True}]],\n",
    "                        shared_xaxes=True,\n",
    "                        vertical_spacing=0.1)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=xs, y=losses, name=\"Training Loss\",line=dict(width=2)),\n",
    "        secondary_y=False, row=1, col=1\n",
    "    )\n",
    "\n",
    "    # fig.add_trace(\n",
    "    #     go.Scatter(x=xs, y=sharpness_H, name=\"Max Eigenvalue of H\", mode='markers', line=dict(width=2)),\n",
    "    #     secondary_y=True, row=1, col=1\n",
    "    # )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=xs, y=sharpness_H, name=\"Max Eigenvalue of A\", mode='markers', line=dict(width=2)),\n",
    "        secondary_y=True, row=1, col=1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=xs, y=test_accuracy, name=\"Test Accuracy\", line=dict(width=2)),\n",
    "        secondary_y=False, row=2, col=1\n",
    "    )\n",
    "\n",
    "    fig.add_hline(y=sharpness_H_lim, line_dash=\"dash\", line_color=\"black\", \n",
    "                  row=1, col=1, secondary_y=True)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Training Loss\", secondary_y=False, \n",
    "                     range = [0,0.5], showgrid=False,\n",
    "                     row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Max Eigenvalue of A\", secondary_y=True, \n",
    "                     range = [0, 5],\n",
    "                     row=1, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"epoch\",\n",
    "                     range = [0,output['train_loss'].isna().sum()])\n",
    "    fig.update_layout(height = 1000, width = 1000)\n",
    "    \n",
    "    fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
